# NPU Benchmark Results: MCXN947 vs STM32N6

_Generated by EAB ML Benchmark Suite — 2026-02-23_

## Test Setup

| Parameter | MCXN947 | STM32N6 |
|-----------|---------|---------|
| Board | FRDM-MCXN947 | NUCLEO-N657X0-Q |
| MCU | Cortex-M33 @ 150 MHz | Cortex-M55 @ 600 MHz |
| NPU | eIQ Neutron NPU | Neural-ART NPU |
| NPU Spec | 2 TOPS (claimed) | 600 GOPS (claimed) |
| Quantization | INT8 | INT8 |
| Memory | 512 KB SRAM | 511 KB axisram2 (SRAM boot) |
| Profiling | DWT cycle counter | DWT cycle counter |
| Boot Method | probe-rs flash (CMSIS-DAP) | SRAM boot via CubeProgrammer + GDB |
| TFLite Micro | Zephyr module (upstream) | Zephyr module (upstream) |

## Measured Results (Real TFLite Micro Inference)

### MCXN947 (150 MHz Cortex-M33, CPU only)

| Model | Backend | Cycles | Time (us) | Ops | Inferences | Notes |
|-------|---------|--------|-----------|-----|------------|-------|
| sine | CPU | 10,789 | 71 | INT8 | 100 | 2,488 byte model, 772/4096 arena |

### STM32N6 (600 MHz Cortex-M55, CPU only)

| Model | Backend | Cycles | Time (us) | Ops | Inferences | Notes |
|-------|---------|--------|-----------|-----|------------|-------|
| sine | CPU | 151,840 | 253 | INT8 | 100 | 2,488 byte model, 784/4096 arena |

**Note**: NPU integration pending on both boards. MCXN947 needs eIQ Neutron delegate; STM32N6 needs STEdgeAI/X-CUBE-AI toolchain.

## Cross-Board Comparison (sine model, CPU-only)

| Metric | MCXN947 | STM32N6 | Ratio |
|--------|---------|---------|-------|
| CPU Clock | 150 MHz | 600 MHz | 4.0x STM32N6 |
| Cycles/inference | 10,789 | 151,840 | 14.1x more on STM32N6 |
| Wall time | 71 us | 253 us | 3.6x slower on STM32N6 |
| Arena usage | 772 bytes | 784 bytes | ~equal |
| Model size | 2,488 bytes | 2,488 bytes | identical |

## Key Findings

1. **MCXN947 Cortex-M33 uses 14x fewer cycles than STM32N6 Cortex-M55** for the same sine model inference. This strongly suggests the TFLite Micro build is not using Cortex-M55 Helium/MVE instructions — the M55 should be more efficient per cycle, not less.
2. **Wall clock: MCXN947 is 3.6x faster** despite 4x lower clock speed (71us vs 253us). The cycle count penalty on M55 outweighs the clock speed advantage.
3. **Likely root cause**: Zephyr's upstream TFLite Micro module may not include CMSIS-NN kernels optimized for Helium. Needs investigation — enabling `CONFIG_CMSIS_NN` and Helium-optimized kernels should dramatically reduce STM32N6 cycles.
4. **Both boards produce real inference**: sin(~6.28) = -0.1694, consistent across both boards (expected ~0.0 for 2*pi, INT8 quantization error is normal).
5. **STM32N6 SRAM boot confirmed working**: Firmware runs from axisram2 (0x34180400) via CubeProgrammer + probe-rs GDB.

## Next Steps

- [ ] Enable CMSIS-NN optimized kernels for STM32N6 (Helium/MVE acceleration)
- [ ] Add person_detect and micro_speech models to both boards
- [ ] Integrate MCXN947 eIQ Neutron NPU delegate
- [ ] Integrate STM32N6 Neural-ART NPU via STEdgeAI
- [ ] Add MobileNet v1 0.25 for larger model benchmarking

## Methodology

- DWT cycle counter for hardware-level profiling (CoreDebug->DEMCR, DWT->CYCCNT)
- INT8 quantized TFLite Micro sine model (Zephyr hello_world sample model)
- 100 inferences averaged per measurement
- Results streamed via EAB serial daemon in `[ML_BENCH]` format
- Both boards captured via UART serial (EAB daemon)

## References

- uNPU-Bench: arxiv.org/abs/2503.22567 (MobiCom 25)
- NXP eIQ Neutron NPU documentation
- STM32Cube.AI Neural-ART documentation
- TFLite Micro Zephyr module: zephyrproject/optional/modules/lib/tflite-micro
